{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55bed25e-b5a9-426b-b279-353dc0a812c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8a065db830>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "from scipy.sparse import csr_matrix\n",
    "    from datetime import datetime\n",
    "# Plotting\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# System\n",
    "from pathlib import Path\n",
    "import os\n",
    "from GraphST import GraphST\n",
    "from GraphST.utils import clustering\n",
    "import itertools\n",
    "\n",
    "import pyreadr\n",
    "\n",
    "# Ensure you are always in the parent dir\n",
    "os.chdir('/home/kyan/git/cv-scdl3991/')\n",
    "# data_path = Path('data/MH/MH_raw_counts.csv')\n",
    "data_path = Path('data/')\n",
    "output_path = Path('outputs/clustering/')\n",
    "# Warnings \n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# setting seed\n",
    "torch.manual_seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb7df85-4641-4e1c-a750-e360371425fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clustering(adata, gt, **kwargs):\n",
    "    \"\"\"\n",
    "    Runs clustering on the AnnData object, computes metrics, and returns results.\n",
    "\n",
    "      Parameters\n",
    "    ----------\n",
    "    adata : AnnData\n",
    "        The single-cell data stored in an AnnData object, which will be used for clustering.\n",
    "    gt : Dataframe containing ground truths\n",
    "    **kwargs : dict, optional\n",
    "        Optional parameters for clustering. The following keys can be passed:\n",
    "        - 'n_clusters' (int): Number of clusters for the clustering algorithm.\n",
    "        - 'radius' (float, default=50): The radius parameter for spatial clustering, if applicable.\n",
    "        - 'tool' (str, default='mclust'): The clustering method to use. Options are 'mclust', 'leiden', or 'louvain'.\n",
    "        - 'refinement' (bool, default=False): Whether to apply a refinement step during clustering.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    adata : AnnData\n",
    "        The modified AnnData object with clustering results and ground truth labels merged in the `obs` DataFrame.\n",
    "    clusters : pd.Series\n",
    "        The series of predicted cluster labels from the clustering algorithm.\n",
    "    gt : pd.Series\n",
    "        The series of ground truth labels for the corresponding cells in `adata`.\n",
    "    \"\"\"\n",
    "\n",
    "    adata = adata.copy()\n",
    "    model = GraphST.GraphST(adata, device=device)\n",
    "    adata = model.train()\n",
    "\n",
    "    n_clusters = kwargs.get('n_clusters')\n",
    "    radius = kwargs.get('radius', 50)\n",
    "    tool = kwargs.get('tool', 'mclust')  # default to 'mclust' if not provided\n",
    "    refinement = kwargs.get('refinement', False)\n",
    "\n",
    "    if tool == 'mclust':\n",
    "       clustering(adata, n_clusters, radius=radius, method=tool, refinement=refinement) # For DLPFC dataset, we use optional refinement step.\n",
    "    elif tool in ['leiden', 'louvain']:\n",
    "       clustering(adata, n_clusters, radius=radius, method=tool, start=0.1, end=2.0, increment=0.01, refinement=False)\n",
    "    \n",
    "    merged_df = pd.merge(adata.obs, gt, left_index=True, right_index=True, how='outer')\n",
    "    adata.obs = merged_df\n",
    "    \n",
    "    # filter for missing values\n",
    "    adata = adata[~pd.isnull(adata.obs['gt'])]\n",
    "    clusters = adata.obs['domain']\n",
    "    gt = adata.obs['gt']\n",
    "\n",
    "    return adata, clusters, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6a87feb-5ec7-4a5d-9615-c1ea1b719429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(name, adata, clusters, gt):\n",
    "    adata = adata[~pd.isnull(adata.obs['gt'])]\n",
    "    X = adata.X\n",
    "    # 6 metrics in total\n",
    "    ARI = metrics.adjusted_rand_score(clusters, gt)\n",
    "    AMI = metrics.adjusted_mutual_info_score(clusters, gt)\n",
    "    HOM = metrics.homogeneity_score(clusters, gt)\n",
    "\n",
    "    # Unsupervised metrics\n",
    "    SIL = metrics.silhouette_score(X, clusters)\n",
    "    CH = metrics.calinski_harabasz_score(X.toarray(), clusters)\n",
    "    DBI = metrics.davies_bouldin_score(X.toarray(), clusters)\n",
    "    \n",
    "    return {name: [ARI, AMI, HOM, SIL, CH, DBI]}, [\"ARI\", \"AMI\", \"HOM\", \"SIL\", \"CH\", \"DBI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c1115ae-4e4d-4928-8951-29c1751d6f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from pathlib import Path\n",
    "\n",
    "def run_all_clustering(adata, gt, **kwargs):\n",
    "    \"\"\"\n",
    "    Runs the clustering process and saves results to disk.\n",
    "    \n",
    "    Args:\n",
    "        adata (AnnData): The AnnData object containing the dataset.\n",
    "        gt (array-like): The ground truth labels for clustering.\n",
    "        **kwargs: Additional keyword arguments for clustering, such as 'n_clusters'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    results_path, adata_path = setup_save_path()\n",
    "\n",
    "    # Run clustering and compute metrics\n",
    "    adata, clusters, gt = run_clustering(adata, gt, **kwargs)\n",
    "    dataset_row, columns = compute_metrics(adata.uns['name'], adata, clusters, gt)\n",
    "\n",
    "    # Prepare the new dataframe for saving\n",
    "    n_clusters = kwargs.get('n_clusters')\n",
    "    new_df = prepare_new_dataframe(dataset_row, columns, n_clusters)\n",
    "\n",
    "    # Load existing results and handle duplication of columns\n",
    "    saved_df = load_existing_results(results_path)\n",
    "    check_duplicate_columns(new_df)\n",
    "\n",
    "    # Combine and save updated results\n",
    "    result = merge_dataframes(saved_df, new_df)\n",
    "    save_results(result, results_path)\n",
    "\n",
    "    # Save the updated AnnData object with clustering results\n",
    "    save_clustering_results(adata, adata_path)\n",
    "\n",
    "def prepare_new_dataframe(dataset_row, columns, n_clusters):\n",
    "    \"\"\"\n",
    "    Prepares the new dataframe to append clustering results.\n",
    "    \n",
    "    Args:\n",
    "        dataset_row (dict): Row of clustering results.\n",
    "        columns (list): List of columns for the dataframe.\n",
    "        n_clusters (int): Number of clusters used for clustering.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The new dataframe with clustering metadata added.\n",
    "    \"\"\"\n",
    "    new_df = pd.DataFrame.from_dict(dataset_row, columns=columns, orient='index')\n",
    "    new_df[\"n_clusters\"] = n_clusters\n",
    "    new_df[\"method\"] = \"GraphST\"\n",
    "    new_df[\"method_type\"] = \"JOINT LOW DIMENSIONAL SPACE DETECTION\"\n",
    "    print(f\"Adding entry {new_df.index} to dataset\")\n",
    "    return new_df\n",
    "\n",
    "def load_existing_results(results_path):\n",
    "    \"\"\"\n",
    "    Loads the saved dataframe from the disk.\n",
    "\n",
    "    Args:\n",
    "        results_path (str or Path): Path to the saved results.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The saved dataframe.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(results_path, index_col=0)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Results file not found at {results_path}. Returning empty dataframe.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def check_duplicate_columns(new_df):\n",
    "    \"\"\"\n",
    "    Checks for and prints warnings if duplicate columns exist in the new dataframe.\n",
    "\n",
    "    Args:\n",
    "        new_df (pd.DataFrame): The new dataframe to check.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    duplicate_columns = new_df.columns[new_df.columns.duplicated()]\n",
    "    if duplicate_columns.any():\n",
    "        print(f\"WARNING: Duplicate columns in {new_df.index}: {duplicate_columns}\")\n",
    "\n",
    "def merge_dataframes(saved_df, new_df):\n",
    "    \"\"\"\n",
    "    Merges the existing dataframe with the new dataframe, ensuring common columns are retained.\n",
    "\n",
    "    Args:\n",
    "        saved_df (pd.DataFrame): The previously saved dataframe.\n",
    "        new_df (pd.DataFrame): The new dataframe to append.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The combined dataframe.\n",
    "    \"\"\"\n",
    "    common_columns = saved_df.columns.intersection(new_df.columns)\n",
    "    print(f\"Common Columns: {common_columns} \\n\")\n",
    "    saved_df_common = saved_df[common_columns]\n",
    "    new_df_common = new_df[common_columns]\n",
    "    return pd.concat([saved_df_common, new_df_common], join=\"inner\")\n",
    "\n",
    "def save_results(result, results_path):\n",
    "    \"\"\"\n",
    "    Saves the combined results to disk.\n",
    "\n",
    "    Args:\n",
    "        result (pd.DataFrame): The final dataframe to save.\n",
    "        results_path (str or Path): Path to save the results.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    save_path = Path(results_path)\n",
    "    result.to_csv(save_path, index=True)\n",
    "    print(f\"Results saved to {save_path}\")\n",
    "\n",
    "def save_clustering_results(adata, adata_path):\n",
    "    \"\"\"\n",
    "    Saves the AnnData object with clustering results.\n",
    "\n",
    "    Args:\n",
    "        adata (AnnData): The AnnData object with clustering results.\n",
    "        adata_path (str or Path): Path to save the AnnData object.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    adata.X = csr_matrix(adata.X)  # Convert dense matrix to sparse format\n",
    "    filename = f\"{adata.uns['name']}_clustering_results.h5ad\"\n",
    "    adata.write_h5ad(Path(adata_path) / filename)\n",
    "    print(f\"AnnData saved to {adata_path}/{filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0f59cf9-c480-422e-a9da-0ecadcb4597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_clustering(adata, gt, **kwargs):\n",
    "    results_path, adata_path = setup_save_path()\n",
    "    \n",
    "    adata, clusters, gt = run_clustering(adata, gt, **kwargs)\n",
    "    dataset_row, columns = compute_metrics(adata.uns['name'], adata, clusters, gt)\n",
    "    n_clusters = kwargs.get('n_clusters')\n",
    "    \n",
    "    ### Adding onto the end of new df\n",
    "    saved_df = pd.read_csv(results_path, index_col = 0)\n",
    "    new_df = pd.DataFrame.from_dict(dataset_row, columns = columns, \n",
    "                                    orient='index')\n",
    "\n",
    "    # Adding clustering metadata\n",
    "    new_df[\"n_clusters\"] = n_clusters\n",
    "    new_df[\"method\"] = \"GraphST\"\n",
    "    new_df[\"method_type\"] = \"JOINT LOW DIMENSIONAL SPACE DETECTION\"\n",
    "    \n",
    "    print(f\"Adding entry < {new_df.index} to dataset\")\n",
    "\n",
    "    ### TESTING FOR DUPLICATED COLUMNS\n",
    "    duplicate_columns = new_df.columns[new_df.columns.duplicated()]\n",
    "    # Display results\n",
    "    if duplicate_columns.any():\n",
    "        print(f\"WARNING: Duplicate columns in {new_df.index}:\")\n",
    "        print(duplicate_columns)\n",
    "\n",
    "    common_columns = saved_df.columns.intersection(new_df.columns)\n",
    "\n",
    "    print(f\"Common Columns: {common_columns} \\n\")\n",
    "    \n",
    "    # Select only the common columns from both DataFrames\n",
    "    saved_df_common = saved_df[common_columns]\n",
    "    new_df_common = new_df[common_columns]\n",
    "\n",
    "    # Append df2 to df1\n",
    "    result = pd.concat([saved_df_common, new_df_common], join=\"inner\")\n",
    "    result.to_csv(save_path, index= True)\n",
    "\n",
    "    print(f\"Saved to disk at path: {save_path_clusters}\")\n",
    "\n",
    "    # Saving adata with clustering results\n",
    "    adata.X = csr_matrix(adata.X)\n",
    "    filename = f\"{adata.uns['name']}_clustering_results.h5ad\"\n",
    "    adata.write_h5ad(adata_path/filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52d61a6e-41c0-419d-98dc-5889709cba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_save_path():\n",
    "    \"\"\"Ensures the save directory exists and returns the save path. Also creates a path to store all the modified anndatas\"\"\"\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d-%m-%H:%M\")\n",
    "    path = f\"outputs/clustering/GraphST-{dt_string}/clustering.csv\"\n",
    "    path1 = f\"outputs/clustering/GraphST-{dt_string}/adatas\"\n",
    "    save_dir = Path(path).parent\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    Path(path1).mkdir(parents=True, exist_ok=True)\n",
    "    return path, path1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "140aee4e-8122-4c66-8db2-1289a1d35476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt_df(path):\n",
    "    gt = pd.read_csv(path, header=None)\n",
    "    gt['gt'] = gt.iloc[:, -1]\n",
    "    gt = gt.set_index(0)\n",
    "    gt = gt[['gt']]\n",
    "    return gt\n",
    "\n",
    "def process_hbca1_gt(path):\n",
    "    df_meta = pd.read_csv(path, sep='\\t')\n",
    "    df_meta.set_index('ID', inplace=True)\n",
    "    df_meta['gt'] = pd.Categorical(df_meta.iloc[:, -1]).codes\n",
    "    return df_meta[['gt']]\n",
    "\n",
    "def process_clusterid_gt(adata):\n",
    "    return pd.DataFrame(adata.obs['ClusterID']).rename(columns={'ClusterID': 'gt'})\n",
    "\n",
    "def load_all_ground_truths(gt_paths, adata_list):\n",
    "    ground_truths = {}\n",
    "\n",
    "    for adata in adata_list:\n",
    "        adata_name = adata.uns['name']\n",
    "        \n",
    "        if adata_name == 'HBCA1':\n",
    "            ground_truths[adata_name] = process_hbca1_gt(gt_paths[adata_name])\n",
    "        elif adata_name.startswith('DPLC'):\n",
    "            ground_truths[adata_name] = get_gt_df(gt_paths[adata_name])\n",
    "        else:\n",
    "            ground_truths[adata_name] = process_clusterid_gt(adata)\n",
    "\n",
    "    return ground_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6df49265-c229-4162-9e21-cc656e2235cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dlpc_ground_truths(dlpc_dir):\n",
    "    dlpc_dir = Path(dlpc_dir)\n",
    "    gt_paths = {}\n",
    "    for folder in dlpc_dir.iterdir():\n",
    "        if folder.is_dir():\n",
    "            patient_id = folder.name\n",
    "            gt_file = folder/ \"gt/tissue_positions_list_GTs.txt\"\n",
    "            \n",
    "            if gt_file.exists():  # Check if the ground truth file exists\n",
    "                gt_paths[patient_id] = str(gt_file)\n",
    "    return gt_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80e36101-6c26-4ad3-959d-0c28e049ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_paths = load_dlpc_ground_truths(data_path/\"DLPC\")\n",
    "gt_paths[\"HBCA1\"] = \"data/HBCA1/gt/gold_metadata.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "914651fe-0b37-48b4-88f8-50d52f09dc42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'151671': 'data/DLPC/151671/gt/tissue_positions_list_GTs.txt',\n",
       " '151509': 'data/DLPC/151509/gt/tissue_positions_list_GTs.txt',\n",
       " '151675': 'data/DLPC/151675/gt/tissue_positions_list_GTs.txt',\n",
       " '151507': 'data/DLPC/151507/gt/tissue_positions_list_GTs.txt',\n",
       " '151669': 'data/DLPC/151669/gt/tissue_positions_list_GTs.txt',\n",
       " '151670': 'data/DLPC/151670/gt/tissue_positions_list_GTs.txt',\n",
       " '151674': 'data/DLPC/151674/gt/tissue_positions_list_GTs.txt',\n",
       " '151676': 'data/DLPC/151676/gt/tissue_positions_list_GTs.txt',\n",
       " '151508': 'data/DLPC/151508/gt/tissue_positions_list_GTs.txt',\n",
       " '151510': 'data/DLPC/151510/gt/tissue_positions_list_GTs.txt',\n",
       " '151673': 'data/DLPC/151673/gt/tissue_positions_list_GTs.txt',\n",
       " '151672': 'data/DLPC/151672/gt/tissue_positions_list_GTs.txt',\n",
       " 'HBCA1': 'data/HBCA1/gt/gold_metadata.tsv'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77d5f1aa-11b9-46f2-8237-488d01bb06e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_MH_datasets(MH_dir):\n",
    "    datasets = []\n",
    "    # Files must follow the naming scheme MH_{sample}.h5ad\n",
    "    for file in MH_dir.glob(\"MH_*.h5ad\"):\n",
    "        print(f\"Loading data from {file.stem}...\")\n",
    "        adata = sc.read_h5ad(file)\n",
    "        adata.uns['name'] = file.stem\n",
    "        datasets.append(adata)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f861c826-31fe-4596-a1a6-66dc3ba131ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mapping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(pd\u001b[38;5;241m.\u001b[39mCategorical(\u001b[43madata\u001b[49m\u001b[38;5;241m.\u001b[39mobs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCell_class\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcategories))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'adata' is not defined"
     ]
    }
   ],
   "source": [
    "mapping = dict(enumerate(pd.Categorical(adata.obs['Cell_class']).categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e954b4ef-d264-4d61-800c-b7ea1d20b929",
   "metadata": {},
   "source": [
    "## Testing workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a568a16-7867-4643-8dbd-2da61e9a148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "# Plotting\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# System\n",
    "from pathlib import Path\n",
    "import os\n",
    "from GraphST import GraphST\n",
    "from GraphST.utils import clustering\n",
    "import itertools\n",
    "\n",
    "import pyreadr\n",
    "\n",
    "# Ensure you are always in the parent dir\n",
    "os.chdir('/home/kyan/git/cv-scdl3991/')\n",
    "data_path = Path('data/')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# setting seed\n",
    "torch.manual_seed(17)\n",
    "\n",
    "\n",
    "# Loading all adatas\n",
    "# Function to load all valid DLPC datasets from the DPLC directory\n",
    "def load_dlpc_datasets(dlpc_dir):\n",
    "    dlpc_dir = Path(dlpc_dir)\n",
    "    datasets = []\n",
    "\n",
    "    # Iterate through all directories in the DPLC folder\n",
    "    for folder in dlpc_dir.iterdir():\n",
    "        if folder.is_dir():  # Check if it's a directory\n",
    "            patient_id = folder.name\n",
    "            count_file = folder / (patient_id + \"_filtered_feature_bc_matrix.h5\")\n",
    "            if count_file.exists():  # Check if the data file exists\n",
    "                print(f\"Loading data from {folder.name}...\")\n",
    "                adata = sc.read_visium(folder, count_file=f\"{patient_id}_filtered_feature_bc_matrix.h5\")\n",
    "                adata.uns['name'] = folder.name\n",
    "                datasets.append(adata)\n",
    "            else:\n",
    "                print(f\"Skipping folder {folder.name}: no valid data file found.\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "\n",
    "def load_MH_datasets(MH_dir):\n",
    "    datasets = []\n",
    "    # Files must follow the naming scheme MH_{sample}.h5ad\n",
    "    for file in MH_dir.glob(\"MH_*.h5ad\"):\n",
    "        print(f\"Loading data from {file.stem}...\")\n",
    "        adata = sc.read_h5ad(file)\n",
    "        adata.uns['name'] = file.stem\n",
    "        datasets.append(adata)\n",
    "    return datasets\n",
    "\n",
    "\n",
    "# Load the specific datasets\n",
    "def load_datasets():\n",
    "\n",
    "    datasets = []\n",
    "\n",
    "    data_path_MSC = Path('data/MSC/MSC_gene_expression_FINAL.h5ad')\n",
    "    adata = sc.read_h5ad(data_path_MSC)\n",
    "    adata.uns['name'] = 'MSC'\n",
    "\n",
    "    data_path_HBCA1 = Path('data/HBCA1/')\n",
    "    adata1 = sc.read_visium(data_path_HBCA1, count_file=\"V1_Breast_Cancer_Block_A_Section_1_filtered_feature_bc_matrix.h5\")\n",
    "    adata1.uns['name'] = 'HBCA1'\n",
    "    \n",
    "    dlpc_adatas = load_dlpc_datasets(data_path/\"DLPC\")\n",
    "    MH_adatas = load_MH_datasets(data_path/\"MH\")\n",
    "\n",
    "    datasets.extend([adata, adata1])\n",
    "    datasets.extend(dlpc_adatas)\n",
    "    datasets.extend(MH_adatas)\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "# Loading all ground truths:\n",
    "def load_dlpc_ground_truths(dlpc_dir):\n",
    "    dlpc_dir = Path(dlpc_dir)\n",
    "    gt_paths = {}\n",
    "    for folder in dlpc_dir.iterdir():\n",
    "        if folder.is_dir():\n",
    "            patient_id = folder.name\n",
    "            gt_file = folder/ \"gt/tissue_positions_list_GTs.txt\"\n",
    "            \n",
    "            if gt_file.exists():  # Check if the ground truth file exists\n",
    "                gt_paths[patient_id] = str(gt_file)\n",
    "    return gt_paths\n",
    "\n",
    "def process_DLPC_gt(path):\n",
    "    gt = pd.read_csv(path, header=None)\n",
    "    gt['gt'] = gt.iloc[:, -1]\n",
    "    gt = gt.set_index(0)\n",
    "    gt = gt[['gt']]\n",
    "    return gt\n",
    "\n",
    "def process_HBCA1_gt(path):\n",
    "    df_meta = pd.read_csv(path, sep='\\t')\n",
    "    df_meta.set_index('ID', inplace=True)\n",
    "    df_meta['gt'] = pd.Categorical(df_meta.iloc[:, -1]).codes\n",
    "    return df_meta[['gt']]\n",
    "\n",
    "def process_MSC_gt(adata):\n",
    "    return pd.DataFrame(adata.obs['ClusterID']).rename(columns={'ClusterID': 'gt'})\n",
    "\n",
    "def process_MH_gt(adata):\n",
    "    adata.obs['gt'] = pd.Categorical(adata.obs['Cell_class']).codes\n",
    "    return adata.obs[['gt']]\n",
    "\n",
    "def load_all_ground_truths(gt_paths, adata_list):\n",
    "    ground_truths = {}\n",
    "\n",
    "    for adata in adata_list:\n",
    "        try:\n",
    "            adata_name = adata.uns['name']\n",
    "            \n",
    "            if adata_name == 'HBCA1':\n",
    "                ground_truths[adata_name] = process_HBCA1_gt(gt_paths[adata_name])\n",
    "            elif adata_name.startswith('DPLC'):\n",
    "                ground_truths[adata_name] = process_DLPC_gt(gt_paths[adata_name])\n",
    "            elif adata_name == 'MSC':\n",
    "                ground_truths[adata_name] = process_MSC_gt(adata)\n",
    "            elif adata_name.startswith('MH'):\n",
    "                ground_truths[adata_name] = process_MH_gt(adata)\n",
    "        except:\n",
    "            print(f\"An error has occurred with file {adata_name}, skipping to next file.\")\n",
    "\n",
    "    return ground_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4ffaab9-cf85-40fe-bf9d-4339056d2c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from 151671...\n",
      "Loading data from 151509...\n",
      "Loading data from 151675...\n",
      "Loading data from 151507...\n",
      "Loading data from 151669...\n",
      "Skipping folder DLPFC12: no valid data file found.\n",
      "Loading data from 151670...\n",
      "Loading data from 151674...\n",
      "Loading data from 151676...\n",
      "Loading data from 151508...\n",
      "Loading data from 151510...\n",
      "Loading data from 151673...\n",
      "Loading data from 151672...\n",
      "Loading data from MH_11...\n",
      "Loading data from MH_5...\n",
      "Loading data from MH_6...\n"
     ]
    }
   ],
   "source": [
    "datasets = load_datasets()\n",
    "gt_paths = load_dlpc_ground_truths(data_path/\"DLPC\")\n",
    "gt_paths[\"HBCA1\"] = \"data/HBCA1/gt/gold_metadata.tsv\"\n",
    "ground_truths = load_all_ground_truths(gt_paths, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e94d05a-ef5d-485d-a4e0-209d4a81882d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSC\n",
      "HBCA1\n",
      "151671\n",
      "151509\n",
      "151675\n",
      "151507\n",
      "151669\n",
      "151670\n",
      "151674\n",
      "151676\n",
      "151508\n",
      "151510\n",
      "151673\n",
      "151672\n",
      "MH_11\n",
      "MH_5\n",
      "MH_6\n"
     ]
    }
   ],
   "source": [
    "for adata in datasets:\n",
    "    print(adata.uns[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2f3708b-256f-4091-ab15-c86512e754be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['MSC', 'HBCA1', 'MH_11', 'MH_5', 'MH_6'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffb7c757-e417-480e-a2c1-24808bdc1423",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1c3d9ca-5bc2-4897-9855-5e4922854c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = len(ground_truths[test.uns[\"name\"]][\"gt\"].unique())\n",
    "gt = ground_truths[test.uns[\"name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1366cb9d-7d23-4c6c-b4a2-b3d663851307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to train ST data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 600/600 [00:04<00:00, 127.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization finished for ST data!\n",
      "fitting ...\n",
      "  |                                                                      |   0%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |======================================================================| 100%\n"
     ]
    }
   ],
   "source": [
    "adata_res, clusters, gt = run_clustering(test, gt, n_clusters=counts, radius=50, refinement= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb312051-1f2e-4c5e-887e-0ae47935ef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_row, columns = compute_metrics(test.uns['name'], adata_res, clusters, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bd20c6d-3b16-425c-9366-c7f9e6560bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path, adata_path = setup_save_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fea8d17-2226-493c-950f-a4d901f0d355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding entry Index(['MSC'], dtype='object') to dataset\n"
     ]
    }
   ],
   "source": [
    "# Prepare the new dataframe for saving\n",
    "n_clusters = counts\n",
    "new_df = prepare_new_dataframe(dataset_row, columns, n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "891d72dd-c41f-470b-a49d-ececfc474530",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a436405-b61f-4555-90ef-59a189a35719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ARI', 'AMI', 'HOM', 'SIL', 'CH', 'DBI', 'n_clusters', 'method',\n",
       "       'method_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eaa15444-e02c-4f58-b970-b07d1040a6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outputs/clustering/GraphST-14-09-21:52/clustering.csv'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d0e878d-18a5-4699-8592-23453c47a70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to outputs/clustering/GraphST-14-09-21:52/clustering.csv\n"
     ]
    }
   ],
   "source": [
    "save_results(new_df, results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cf0d3ba-a8c6-4852-b1a9-5afadc29229b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData saved to outputs/clustering/GraphST-14-09-21:52/adatas / MSC_clustering_results.h5ad\n"
     ]
    }
   ],
   "source": [
    "# Save the updated AnnData object with clustering results\n",
    "save_clustering_results(test, adata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aef2744-3599-454c-b137-a381f7b3bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing results and handle duplication of columns\n",
    "saved_df = load_existing_results(results_path)\n",
    "check_duplicate_columns(new_df)\n",
    "\n",
    "# Combine and save updated results\n",
    "result = merge_dataframes(saved_df, new_df)\n",
    "\n",
    "# Save the updated AnnData object with clustering results\n",
    "save_clustering_results(adata, adata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4aec5b1b-d0ce-42a3-ab92-66c286d800ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['ARI', 'AMI', 'HOM', 'SIL', 'CH', 'DBI', 'n_clusters', 'method', 'method_type']\n",
    "\n",
    "# Create an empty DataFrame with the specified columns\n",
    "empty_df = pd.DataFrame(columns=columns)\n",
    "empty_df.to_csv(results_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6bddb9e2-4043-4dde-82ef-7fed680ac8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_df.to_csv(\"test\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c92b0e26-54cf-40d8-a6b9-07c205d9fd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "6\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "for i in range(4, 10, 2):\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
